#!/bin/bash
#-- NOT A REAL SHELL SCRIPT!   This is a cut'n'paste scriptlet!
#   Be sure to read & understand what you're cutting & pasting!
#------------------------------------------------------------------------------
# name:    OKD (OpenShift Origin) Notes (miscNotes)
# author:  awmyhr@gmail.com
# revised: 20180822-164710
# created: 2018-08-10
#------------------------------------------------------------------------------
#-- Minishift stuff
#------------------------------------------------------------------------------
# OpenShift server start/stop.
minishift [start|stop]
# Get minishift IP address
minishift ip
# Get minishift env (PATH) settings
minishift oc-env
# The server is accessible via web console at:
#     https://192.168.42.175:8443
# To login in as developer
oc login -u developer:developer
# To login as administrator:
oc login -u system:admin
#------------------------------------------------------------------------------
#-- "Quick" All-in-one binary download install
#------------------------------------------------------------------------------
RELEASE='v3.10.0'
BUILD='dd10d17'
PLATFORM='linux-64bit'
wget -O /var/tmp/open-shift-server.tgz "https://github.com/openshift/origin/releases/download/${RELEASE}/openshift-origin-server-${RELEASE}-${BUILD}-${PLATFORM}.tar.gz"
cd /opt
tar zxf /var/tmp/open-shift-server.tgz
ln -s "openshift-origin-server-${RELEASE}-${BUILD}-${PLATFORM}" openshift
export PATH=/opt/openshift:"${PATH}"
cd "${HOME}"
# (as root)
openshift start >var/log/openshift.log 2>&1 &
export KUBECONFIG="${HOME}"/openshift.local.config/master/admin.kubeconfig
export CURL_CA_BUNDLE="${HOME}"/openshift.local.config/master/ca.crt
chmod +r "${HOME}"/openshift.local.config/master/admin.kubeconfig

#------------------------------------------------------------------------------
#-- "Full" Ansible-based install prep
#   See also Ansible playbook 'openShiftPrep.yml'
#   This smooths over some common issues I've
#   seen with running the Ansible playbooks.
#------------------------------------------------------------------------------
#-- Filesystems
#-- (maybe) make a docker-vg (this may override /var/lib/docker)
#   /var (and sub-fs) itself should have total of 5GB+ before below
#   /tmp and /usr/local/bin should have 1GB+ free
#-- An additional minimum 15 GB unallocated space per system running containers
#   for Dockerâ€™s storage back end; see Configuring Docker Storage. Additional
#   space might be required, depending on the size and number of containers that
#   run on the node.
mkdir /var/lib/docker /var/lib/origin
chown root:root /var/lib/docker /var/lib/origin
chmod 0755 /var/lib/docker /var/lib/origin
lvcreate --yes -L 30G -n var.lib.docker vg01 && mkfs.xfs /dev/mapper/vg01-var.lib.docker
lvcreate --yes -L  5G -n var.lib.origin vg01 && mkfs.xfs /dev/mapper/vg01-var.lib.origin
cat >>/etc/fstab <<EOF
/dev/mapper/vg01-var.lib.docker /var/lib/docker   xfs     defaults,relatime       0 0
/dev/mapper/vg01-var.lib.origin /var/lib/origin   xfs     defaults,relatime       0 0
EOF
mount /var/lib/docker
mount /var/lib/origin

#-- On NFS node (if needed)
mkdir /exports
chown root:root /exports
chmod 0755 /exports
lvcreate --yes -L 30G -n exports vg01 && mkfs.xfs /dev/mapper/vg01-exports
cat >>/etc/fstab <<EOF
/dev/mapper/vg01-exports        /exports          xfs     defaults,relatime       0 0
EOF
mount /exports

groupadd -g 980 docker
#------------------------------------------------------------------------------
#-- For RHEL -- additional installs before playbooks run
yum install -y git iptables-services pyOpenSSL python-cryptography
yum-config-manager -q --enable rhel-7-server-ansible-2-rpms >/dev/null
yum install -y ansible
yum-config-manager -q --enable rhel-7-server-extras-rpms >/dev/null
yum install -y atomic cockpit-docker docker python-crypto
#-- For CentOS
yum install -y epel-release
yum install -y ansible git docker python-cryptography python-crypto pyOpenSSL
#------------------------------------------------------------------------------
#-- End Install prep
#------------------------------------------------------------------------------
#-- Docker should set this on start
sysctl -w net.ipv4.ip_forward=1
#-- From what I can tell these are just annoying to see, not impactful
# sysctl -w net.bridge.bridge-nf-call-iptables=1
# sysctl -w net.bridge.bridge-nf-call-ip6tables=1

#------------------------------------------------------------------------------
#-- Ansible based installer for okd
git clone https://github.com/openshift/openshift-ansible.git
cd openshift-ansible && git checkout --track origin/release-3.10 && cd ..
cp openshift-ansible/inventory/hosts.example osa-hosts

#-- Customize osa-hosts
#   Here are the vars I've worked with
# [OSEv3:vars]
# debug_level=2
# ansible_user=root
# os_firewall_use_firewalld=True
# openshift_deployment_type=origin
# openshift_release="3.10"
# openshift_portal_net=172.30.0.0/16
# osm_cluster_network_cidr=10.128.0.0/14
# #-- This sets up basic htpasswd authentication
# openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider',}]
# #-- Some environments will need to set insecure registries
# openshift_docker_insecure_registries=172.17.0.0/16,172.30.0.0/16,quay.io,docker.io,registry.access.redhat.com

ansible-playbook -i osa-hosts openshift-ansible/playbooks/prerequisites.yml
ansible-playbook -i osa-hosts openshift-ansible/playbooks/deploy_cluster.yml

#-- or run against localhost
# ansible-playbook -i openshift-ansible/inventory/hosts.localhost openshift-ansible/playbooks/prerequisites.yml
# ansible-playbook -i openshift-ansible/inventory/hosts.localhost openshift-ansible/playbooks/deploy_cluster.yml

#------------------------------------------------------------------------------
#-- Post install quick cheks
#------------------------------------------------------------------------------
#-- Check nodes are running
oc get nodes
#-- list all currently running pods
oc get pods --all-namespaces -o wide
#-- Get status
oc status
#-- Make sure we have NTP
timedatectl
#-- Get current entropy, should be over 1000, advise to alert if under 800
cat /proc/sys/kernel/random/entropy_avail
#-- Check default storage class
oc get storageclass

#-- login as system:admin (aka superuser) while os user is root
oc login -u system:admin
#------------------------------------------------------------------------------
#-- Registry schtuff
#------------------------------------------------------------------------------
oc logs dc/docker-registry
oc describe svc/docker-registry
oc describe dc/docker-registry
oc describe pod "$(oc get pods --no-headers | grep docker-registry | cut -f1 -d' ')"
docker ps --filter=name=registry_docker-registry.*_default_
docker login "$(docker ps -q --filter=name=registry_docker-registry.*_default_)"

docker login -u openshift -p "$(oc whoami -t)" "${DR_IP}:${DR_PORT}"

#-- add registry permissions
oc policy add-role-to-user registry-viewer "${USER}"
oc policy add-role-to-user registry-editor "${USER}"


#------------------------------------------------------------------------------
#--  Sample application
#------------------------------------------------------------------------------
oc new-project test-project
# Create a Node.js example app:
oc new-app https://github.com/openshift/nodejs-ex -l name=myapp
# Track the build log until the app is built and deployed:
oc logs -f bc/nodejs-ex
# Expose a route to the service:
oc expose svc/nodejs-ex
# Access the application:
minishift openshift service nodejs-ex --in-browser
# scale it
oc scale --replicas=2 dc nodejs-ex
# get service/dc/bc info (these can be 'resource name' or 'resource/name')
oc get svc nodejs-ex
oc get dc nodejs-ex
oc get bc nodejs-ex
# more detailed info
oc describe svc nodejs-ex
# delete the project
oc delete project network-test

#------------------------------------------------------------------------------
#-- Some Openshift stuff
#------------------------------------------------------------------------------
# list pods in current project
oc get pods [-o wide]
# status of current project
oc status
# get info on 'everything' in project
oc get all
# log into a pod
oc rsh [podname]

