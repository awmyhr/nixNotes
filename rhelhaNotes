#!/bin/bash NOT A REAL SHELL SCRIPT!
# This is a cut'n'paste scriptlet
# be sure to read & understand what you're cutting & pasting!
#
################################################################################
####  This document focuses on RHEL 6 HA running on VMWare
####     See also: https://access.redhat.com/articles/1351733 
################################################################################
# In VMWare, disks that are to be shared need to be on a "physical" or "virtual"
#   SCSI controller (depending on if you wish them to reside on different hosts)
#  [[
#    vm's need these settings added to Options->General->Configuration Paramaters
#       scsi1:0.sharing = "multi-writer"
#       scsi2:0.sharing = "multi-writer"
#    for each of the controller/disk pairs that are shared.
#  ]]
# Set up the volume group(s) as normal, but then turn it off
#   (only needs to be done on one node):
vgchange -an ${VG}
# volume creation is slightly different, in that lvcreate needs some flags:
lvcreate -n ${LVOL} -L ${LVOLSIZE} ${VG} --addtag pacemaker --config 'activation { volume_list = [ "@pacemaker" ] }'
# On each node nable HA-LVM
lvmconf --enable-halvm --services --startstopservices
# On each node specify local volume groups
vi /etc/lvm/lvm.conf
# Set volume list parameter (local volumes only!). Find this line and modify:
#   volume_list=["${VG1}","${VGX}"]
# rebuild initramfs and reboot (assuming you running the current kernel)
dracut -H -f 
reboot
################################################################################
# Don't forget to create the mount points on all nodes
#   The mountpoints should be "chown root:root" and "chmod 755"
#   this will help ensure no data is written to the non-active node
#
# Each node should have static IPs, and all nodes should be listed
#   in all /etc/hosts files (But NOT as an alias to localhost)
################################################################################
# Here is where actual cluster install/config starts
#   Each node should have access to a RHEL HA repository
#   For example "rhel-x86_64-server-ha-6" on a Satellite server
#
yum -y install pacemaker cman pcs fence-agents
chkconfig corosync off
#
#add hacluster to /etc/security/access.conf
sed -i -e '/EXCEPT/ s/root/root hacluster/' /etc/security/access.conf;
echo ${PASSWD} | passwd --stdin hacluster
chkconfig pcsd on && service pcsd start
pcs cluster auth ${NODE0} ${NODE0} ${NODEX} 
#
# In some cases multicast must be turned off and Unicast should be used
#   set "cman trasnport="udpu" in the line:
#   <cman broadcast="no" expected_votes="1" transport="udpu" two_node="1"/>
sed -i -e '/EXCEPT/ s/udp/udpu/' /etc/cluster/cluster.conf
################################################################################
# Setup and start cluster
pcs cluster setup [--two_node] --name ${CLUSTERNAME} ${NODE0} ${NODEX} 
pcs cluster enable --all && pcs cluster start --all
#
# Create a vmware fence
#   (NOTE: word immediately after "create" is name of resource)
pcs stonith create ${NAME_FENCE} fence_vmware_soap \
    pcmk_host_map="${NODE0}:${NODE0_VMW};${NODEX}:${NODEX_VMW}" \
    ipaddr=${VMWARE_FQDN} login="${VMWARE_LOGIN}" passwd="${VMWARE_PASSWD}" \
    ssl=1  ssl_insecure=1   action=off             pcmk_reboot_timeout=60 \
    pcmk_monitor_timeout=60 pcmk_status_timeout=60 pcmk_list_timeout=60 \
    pcmk_off_timeout=60     pcmk_on_timeout=60
# Though normally one would run a stonith device on each node, these documents:
#  https://access.redhat.com/solutions/917813
#  https://access.redhat.com/solutions/667443
# Imply that for fence_vmware_soap this may not be a good idea
#
# Create other resources; This is just a template, you are very likley to have
#   several differant VG and FS resources.
#   (NOTE: the resource group is auto-created when ${NAME_GROUP} is referneced)
pcs resource create ${NAME_VIP} IPaddr2 ip=${VIP} cidr_netmask=${VIP_CIDR} --group ${NAME_GROUP}
pcs resource create ${NAME_VG}  LVM volgrpname=${VG} exclusive=true --group ${NAME_GROUP}
pcs resource create ${NAME_FS}  Filesystem device=${FS_DEVPATH} directory=${FS_MOUNTPOINT} fstype=${FS_TYPE} --group ${NAME_GROUP}
#
# This will send e-mails whenever a resource starts/stops
#   mailx should be installed and configured for this to work
pcs resource create ${NAME_MAILTO} MailTo email=${EMAIL} subject=${SUBJECT} --group ${NAME_GROUP}
#
# A group implies a sequential, require-all, coloctoin constraint
#   with resource sequence being the same order they are added in. 
#   For refernece here is how to create such a constraint
pcs constraint colocation set ${NAME_VIP} ${NAME_VG} ${NAME_FS} sequential=true require-all=true
#
# Set a cluster property
pcs property set ${PROPERTY}=${VALUE}
#
# This will determine which node the resource wants to run on
pcs constraint location ${NAME_GROUP} prefers ${NODE0}=200
pcs constraint location ${NAME_GROUP} prefers ${NODEX}=100
# Unless you set up stickiness, unnecessary relocations can occure
#   to avoid, set stickiness on resource group (or just set the default
#   though I prefer the explicitness of setting it on the group)
pcs resource meta ${NAME_GROUP} resource-stickiness=1000
pcs resource defaults resource-stickiness=1000
################################################################################
# Common commands to check cluster
# Get status
pcs [cluster|groups|resources|nodes|corosync|pcsd] status [--full]
# Show fence devices/status
pcs stonith show [--full]
# Verify configuratoin of running cluster, no output == good
#   NOTE: This is the same as "crm_verify -L -V"
pcs cluster verify [-V]
# Show information about resources
pcs resource show [${NAME_RESOURCE}|${NAME_GROUP}]
# Show list of constraints
pcs constraint show
# Display status of rings
corosync-cfgtool -s
# Show quorum status
corosync-quorumtool -s
# Show nodes participating in quorum
corosync-quorumtool -l
# Show cluster properties
pcs property list --all
################################################################################
#Common commands to control cluster
# Put a node in standby -- it will not host any resources
pcs cluster standby ${NODEX}
# Undo above
pcs cluster unstandby ${NODEX}
# control status of resource (groups)
pcs resource [enable|disable] ${NAME_RESOURCE}|${NAME_GROUP}
# move resource group to nodeX. the optional lifetime parameter says how long
#   the resource stays on nodeX before being free to move
#   Note: this creates a temporary constraint (see 'pcs constraint show')
pcs resource move ${NAME_GROUP} ${NODEX} [lifetime=${VALUE}]
# Clear temporary constraints
pcs resource clear ${NAME_GROUP}
# If need to reset failcount
pcs resource failcount reset ${NAME_RESOURCE}
# To set which node a resource group prefers
pcs constraint location ${NAME_GROUP} prefers ${NODEX}
# Set resource defaults
#   Example: pcs resource op defaults timeout=60
pcs resource op defaults ${OPTION}=${VALUE}
# Set specific options for specific resources
#   Example: pcs resource update oraresource op start timeout=40
pcs resource update ${NAME_RESOURCE} op ${SETTING} ${OPTION}=${VALUE}
################################################################################
# create resource to run on every node
pcs resource clone ${NAME_RESOURCE}
pcs resource create ${PARAMATERS} --clone
################################################################################
#Additional steps for custom resources:
# Create <custom> resource directory, copy file in, set permissions/ownership
# Note: typically ${CUSTOM} is company/organization name
#   and ${RESOURCE_SCRIPT} is same as the desired resource type name
mkdir /usr/lib/ocf/resource.d/${CUSTOM}
cp    ${RESOURCE_SCRIPT} /usr/lib/ocf/resource.d/${CUSTOM}/${RESOURCE_SCRIPT}
chown root:root /usr/lib/ocf/resource.d/${CUSTOM}/${RESOURCE_SCRIPT}
chmod 755 /usr/lib/ocf/resource.d/${CUSTOM}/${RESOURCE_SCRIPT}
# Check if custom resource exists
pcs resource list | grep -i ${RESOURCE_SCRIPT}
pcs resource describe ocf:${CUSTOM}:${RESOURCE_SCRIPT}
# create resource, but don't monitor
pcs resource create ${NAME_RESOURCE} ${RESOURCE_SCRIPT} op monitor enabled=false on-fail=ignore --group ${NAME_GROUP}
################################################################################
#Controlling the cluster daemons:
# on/start
chkconfig pcsd on       && service pcsd start
chkconfig pacemaker on  && service pacemaker start
service pcsd start      && service pacemaker start
# off/stop
chkconfig pacemaker off && service pacemaker stop
chkconfig pcsd off      && service pcsd stop
service pacemaker stop  && service pcsd stop
################################################################################
#Example resource creation for an Oracle cluster
# Note: resources added in a group are tacked onto the end
#       group implies colocation, order, and required all
# VIP
pcs resource create oravip       IPaddr2    ip=192.168.100.200 cidr_netmask=24 --group oracle
# Filesystems
pcs resource create oracvg01     LVM        volgrpname=cvg01 exclusive=true --group oracle
pcs resource create oracvg01_u00 Filesystem device=/dev/cvg01/u00 directory=/u00 fstype=ext4 --group oracle
pcs resource create oracvg02     LVM        volgrpname=cvg02 exclusive=true --group oracle
pcs resource create oracvg02_u01 Filesystem device=/dev/cvg02/u01 directory=/u01 fstype=ext4 --group oracle
pcs resource create oracvg02_u02 Filesystem device=/dev/cvg02/u02 directory=/u02 fstype=ext4 --group oracle
pcs resource create oracvg02_u03 Filesystem device=/dev/cvg02/u03 directory=/u03 fstype=ext4 --group oracle
pcs resource create oracvg02_u04 Filesystem device=/dev/cvg02/u04 directory=/u04 fstype=ext4 --group oracle
pcs resource create oracvg03     LVM        volgrpname=cvg03 exclusive=true --group oracle
pcs resource create oracvg03_u98 Filesystem device=/dev/cvg03/u98 directory=/u98 fstype=ext4 --group oracle
pcs resource create oracvg03_u99 Filesystem device=/dev/cvg03/u99 directory=/u99 fstype=ext4 --group oracle
# Database built-in resources
pcs resource create lsn-cbdhtest oralsnr    sid=MYDB home=/u00/app/oracle/product/12.1.0.2 user=oracle  --group oracle
pcs resource create odb-cbdhtest oracle     sid=MYDB home=/u00/app/oracle/product/12.1.0.2 user=oracle  --group oracle
# Stickiness, to keep the cluster from automagically moving when "primary" node
#   comes on-line
pcs resource defaults resource-stickiness=1000
pcs constraint location oracle prefers ${NODE0}=200
pcs constraint location oracle prefers ${NODEX}=100
################################################################################
# Misc info
#   Information regarding db2 resource:l
#       http://www.linux-ha.org/wiki/Db2_(resource_agent)
################################################################################

